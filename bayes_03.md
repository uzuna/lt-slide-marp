---
marp: true
theme: default
pagination: true
---

# ベイズ統計入門 Vol.3

---

## 確率分布

1/2 の意味するところ

---

### ベルヌーイ試行

コインを n 回投げて k 回成功する(=表が出る)例を考える。

${}_nC_k = (a + b)^n$の組み合わせ。パスカルの三角形

$$
1\\
1\space 1\\
1\space 2\space 1\\
1\space 3\space 3\space 1\\
1\space 4\space 6\space 4\space 1\\
1\space 5\space 10\space 10\space 5\space 1\\
$$

---

1 段ごとに組み合わせの数は$2^n$で増えるので、
全体を 1 としたときの組み合わせの面積は$\frac{1}{2}^n$

n 回投げた時の可能性すべてを足して 1 になるようにすると

${}_nC_k \frac{1}{2}^n$

---

### もし成功と失敗の確率が均等でない場合の面積は?

- 公正なコインの表を成功とする: $\theta= \frac{1}{2}$
  $\frac{1}{2}^n = \frac{1}{2}^k (1-\frac{1}{2})^{n-k}$

- 公正 10 面サイコロの内 1 を成功とする: $\theta= \frac{1}{10}$
  $\frac{1}{10}^k (1-\frac{1}{10})^{n-k}$

---

## 二項分布

${}_nC_k \theta^k(1-\theta)^{n-k}$

ガチャ排出確率 1,5,10%で 10 回試行したときに 1 つでも出る確率
${}_{10}C_1 0.01^1(1-0.01)^{10-1} = 0.0913$
${}_{10}C_1 0.05^1(1-0.05)^{10-1} = 0.3151$
${}_{10}C_1 0.1^1(1-0.1)^{10-1} = 0.38742$

---

二項分布の空間的な特性

1. ${}_nC_k$ 組み合わせの数は増えていく
1. ひとつ当たりの面積は指数的に比率が小さくなる$\theta^k(1-\theta)^{n-k}$

$Bin(n, \theta) = {}_nC_k \theta^k(1-\theta)^{n-k}$

![](img/03_binorm.png)

---

ここに試行データ n 回投げて k 回成功がある。
この時適切な$\theta$は何か?

最尤推定: $argmax(L(\theta))$となる$\theta$を選ぶ

最も高いのは期待値$n\theta=\frac{k}{n}$

---

ベルヌーイ分布は 1 回の試行で表が出るかどうかを知る確率分布
二項分布の n=1 の特別な形

$$
\begin{aligned}
Bin(n, \theta) &= {}_nC_k \theta^k(1-\theta)^{n-k}\\
Bern(\theta) &= \theta^k(1-\theta)^{1-k}
\end{aligned}
$$

---

## ベイズの定理、確率分布編

ある試行が 10 回中 8 回成功した。
この時の事後確率を求めよ

---

前提

- 観測データは未知の$\theta$パラメータを持つ分布から得られたとする
- 同じ分布から予測を取り出すことができる

![](img/03_bern_gm.png)

---

謎の分布$\theta$から得たデータは 10 回中 8 回の尤度は以下のようになる

$z=\lbrace 1,0,1,1,1,1,1,0,1,1\rbrace$

$P(z|\theta) = \theta^8(1-\theta)^2$

---

尤度の形に合わせて、事前確率にベータ関数を用いる

$p(\theta) = Beta(a,b) = \theta^{a-1}(1-\theta)^{b-1}$

$a,b$はハイパーパラメータ。
ベータ関数はパラメータ a,b から$\theta = (0, 1)$を生成する関数
ここでは何も知らない状態として $a=1, b=1$を置く

ベータ分布の期待値 $\frac{a}{a+b}$

---

ベイズの定理に合わせて展開すると

$p(\theta|z) \propto P(z|\theta)p(\theta) = \theta^8(1-\theta)^2 \theta^{1-1}(1-\theta)^{1-1}$

これを整理すると

$p(\theta|z) = \theta^8(1-\theta)^2$ というベータ分布の事後確率が得られる
![](img/03_beta_11_82.png)

期待値(平均)は$\frac{a}{a+b} = \frac{8}{10} = 0.8$

---

指数を展開することなく事後確率 $Beta(\hat{a}, \hat{b}) = Beta(8,2)$が得られた。

このような尤度と事前事後分布の関係を持つ分布を共役事前分布と呼ぶ

---

共役事前分布

- ベータ分布 $Beta(a, b) -> \theta $
  - ベルヌーイ分布
  - 多試行化 -> 二項分布
- 多次元化 -> ディリクレ分布 $Fir(\alpha_k) -> \pi_k $
  - カテゴリ分布
  - 多試行化 -> 多項分布

---

### 事前分布とデータの数

先ほどは何も知らないというデータを置いた
$p(\theta) = Beta(1,1)$

過去の経験を通してコイン平均 0.5 で分散 0.05 ぐらいじゃないかなー
$p(\theta) = Beta(50,50)$

なんとしても裏が出てほしいお気持ち
$p(\theta) = Beta(1,99)$

---

結果
事前の信念が大きいほど、影響は小さくなる
![](img/03_prep_11_5050.png)

---

データが圧倒的にあれば?

謎の分布$\theta$から得たデータは 1000 回中 800 回とする

![](img/03_postp_1000.png)

データの比率が大きくなって観測値に収束していく

---

注意

- 計算上パラメータ a,b の加算で求められるのは、事後の確率分布がベータ関数という前提で確率推論から明らかにしただけ
- 事後分布は a,b が直接更新されるというわけではない
- このようにハイパーパラメータを直接更新するのは**経験ベイズ法**と呼ばれる
- 共役でない場合は実際にサンプリングをして分布を求める
  - 例 MCMC(マルコフ連鎖モンテカルロ法)

---

ベイズ決定

$\theta$を推定した後にどう使うのか

---

損失関数 $L(\theta|a)$ を作る

事後分布から代表値を取り出す

- median
- mean
- mode, MAP 推定量(Maximum a posteriori)と呼ばれる
- percentile: 過大推定、過小推定にペナルティを設ける

---

事後確率`Beta(10,20)`ゲームに、任意の個体は参加するかどうか

$p(\theta|z) = \theta^{10}(1-\theta)^{20}$
これは展開すると次の予測分布が得られる。
これに何を使うかが意思決定を左右する

- MAP 推定 $p(x) = Bern(x_*|\frac{a}{a+b})$
- Mode 推定 $p(x) = Bern(x_*|\frac{a-1}{a+b-2})$

---

重みの違いを見せるほうがいいか?
MAP 推定後のそれの閾値を使うとか?

いずれにしても学習はデータのみによって行い、
その結果をどのように決定に使うか解釈ができる。

---

## 実例

スペースシャトルの例

---

O-Ring 破損による事故

低温化すると脆弱になるという性質をロジスティック関数で近似

$y=\frac{1}{1+e^{-(bx+a)}}$

![](img/03_logistic_curve.png)

---

グラフィカルモデリング

データ x の成否とその温度情報を使って
ロジスティック関数のパラメータ a,b を推定する

![](img/03_challenger_gm.png)

---

PyMC3 を使って MCMC で推測した後のパラメータの分布
![](img/03_challenger_trace_graph.png)

---

![](img/03_challenger_result.png)

---

次回

- 線形回帰
- カルマンフィルター
- MCMC
